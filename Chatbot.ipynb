{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from builtins import open, bytes\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "import math as math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "##import Tensorflow namespaces\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.keras.layers import Embedding, Activation, Dense, Dropout, TimeDistributed, Dense, Activation, LSTM\n",
    "#import tensorflow.keras.utils as kutils\n",
    "#\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from tensorflow.keras.models import load_model\n",
    "#\n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "#\n",
    "## GPU\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###### References ######\n",
    "########################\n",
    "\n",
    "# Some of the code is copied or derived from this notebook: https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG\n",
    "# This was made by Rostyslav Neskorozhenyi (https://www.linkedin.com/in/slanj/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Basic functions #\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arr(arr):\n",
    "    print(\"size = \" + str(len(arr)))\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(text):\n",
    "    width = len(text) + 6\n",
    "    border = \"\"\n",
    "    for i in range(width):\n",
    "        border += \"-\"\n",
    "    \n",
    "    print(border)\n",
    "    print(\"--\", text, \"--\")\n",
    "    print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Get data #\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ep4 = pd.read_csv('Data/star_wars_epIV.txt', sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep5 = pd.read_csv('Data/star_wars_epV.txt', sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep6 = pd.read_csv('Data/star_wars_epVI.txt', sep =' ', header=0, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>Did you hear that?  They've shut down the main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>We're doomed!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>There'll be no escape for the Princess this time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>What's that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>I should have known better than to trust the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Oh, no!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>Oh, my!  Artoo!  Can you hear me?  Say somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>TECHNICIAN</td>\n",
       "      <td>We'll get to work on him right away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>You must repair him!  Sir, if any of my circui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>He'll be all right.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       character                                           dialogue\n",
       "1       THREEPIO  Did you hear that?  They've shut down the main...\n",
       "2       THREEPIO                                      We're doomed!\n",
       "3       THREEPIO  There'll be no escape for the Princess this time.\n",
       "4       THREEPIO                                       What's that?\n",
       "5       THREEPIO  I should have known better than to trust the l...\n",
       "...          ...                                                ...\n",
       "1006        LUKE                                            Oh, no!\n",
       "1007    THREEPIO  Oh, my!  Artoo!  Can you hear me?  Say somethi...\n",
       "1008  TECHNICIAN               We'll get to work on him right away.\n",
       "1009    THREEPIO  You must repair him!  Sir, if any of my circui...\n",
       "1010        LUKE                                He'll be all right.\n",
       "\n",
       "[1010 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ep4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Echo Three to Echo Seven. Han, old buddy, do y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAN</td>\n",
       "      <td>Loud and clear, kid. What's up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Well, I finished my circle. I don't pick up an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAN</td>\n",
       "      <td>There isn't enough life on this ice cube to fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Right. I'll see you shortly. There's a meteori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>I'll meet you at the rendezvous point on  Tato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>LANDO</td>\n",
       "      <td>Princess, we'll find Han. I promise.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Chewie, I'll be waiting for your signal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Take care, you two. May the Force be with you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Ow!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    character                                           dialogue\n",
       "1        LUKE  Echo Three to Echo Seven. Han, old buddy, do y...\n",
       "2         HAN                    Loud and clear, kid. What's up?\n",
       "3        LUKE  Well, I finished my circle. I don't pick up an...\n",
       "4         HAN  There isn't enough life on this ice cube to fi...\n",
       "5        LUKE  Right. I'll see you shortly. There's a meteori...\n",
       "..        ...                                                ...\n",
       "835      LUKE  I'll meet you at the rendezvous point on  Tato...\n",
       "836     LANDO               Princess, we'll find Han. I promise.\n",
       "837      LUKE           Chewie, I'll be waiting for your signal.\n",
       "838      LUKE     Take care, you two. May the Force be with you.\n",
       "839      LUKE                                                Ow!\n",
       "\n",
       "[839 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ep5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHUTTLE CAPTAIN</td>\n",
       "      <td>Command station, this is ST 321. Code Clearanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEATH STAR CONTROLLER</td>\n",
       "      <td>The security deflector shield will be deactiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHUTTLE CAPTAIN</td>\n",
       "      <td>We're starting our approach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OFFICER</td>\n",
       "      <td>Inform the commander that Lord Vader's shuttle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPERATOR</td>\n",
       "      <td>Yes, sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>LANDO</td>\n",
       "      <td>Wedge, I don't think we're going to make it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>WEDGE</td>\n",
       "      <td>You'll make it. Just follow me Gold Leader.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>LANDO</td>\n",
       "      <td>I promised to return his ship without a scratc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>HAN</td>\n",
       "      <td>Lando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>They did it!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 character                                           dialogue\n",
       "1          SHUTTLE CAPTAIN  Command station, this is ST 321. Code Clearanc...\n",
       "2    DEATH STAR CONTROLLER  The security deflector shield will be deactiva...\n",
       "3          SHUTTLE CAPTAIN                       We're starting our approach.\n",
       "4                  OFFICER  Inform the commander that Lord Vader's shuttle...\n",
       "5                 OPERATOR                                          Yes, sir.\n",
       "..                     ...                                                ...\n",
       "670                  LANDO       Wedge, I don't think we're going to make it.\n",
       "671                  WEDGE        You'll make it. Just follow me Gold Leader.\n",
       "672                  LANDO  I promised to return his ship without a scratc...\n",
       "673                    HAN                                           Lando...\n",
       "674               THREEPIO                                       They did it!\n",
       "\n",
       "[674 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ep6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_ep4 = df_ep4.character.unique()\n",
    "characters_ep5 = df_ep5.character.unique()\n",
    "characters_ep6 = df_ep6.character.unique()\n",
    "#print_arr(characters_ep4)\n",
    "#print_arr(characters_ep5)\n",
    "#print_arr(characters_ep6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "-- All characters --\n",
      "--------------------\n",
      "size = 129\n",
      "['ACKBAR' 'ANAKIN' 'ANNOUNCER' 'ASSISTANT OFFICER' 'ASTRO-OFFICER'\n",
      " 'AUNT BERU' 'BARTENDER' 'BASE VOICE' 'BEN' 'BERU' 'BIB' 'BIGGS'\n",
      " 'BOBA FETT' 'BOUSHH' 'BUNKER COMMANDER' 'CAMIE' 'CAPTAIN' 'CHIEF'\n",
      " 'CHIEF PILOT' 'COMMANDER' 'COMMUNICATIONS OFFICER' 'CONTROL OFFICER'\n",
      " 'CONTROL ROOM COMMANDER' 'CONTROLLER' 'CREATURE' 'DACK' 'DEAK'\n",
      " 'DEATH STAR CONTROLLER' 'DEATH STAR INTERCOM VOICE' 'DECK OFFICER'\n",
      " 'DERLIN' 'DODONNA' 'EMPEROR' 'FIRST CONTROLLER' 'FIRST OFFICER'\n",
      " 'FIRST TROOPER' 'FIXER' 'GANTRY OFFICER' 'GENERAL MADINE' 'GOLD FIVE'\n",
      " 'GOLD LEADER' 'GOLD TWO' 'GRAY LEADER' 'GREEDO' 'GREEN LEADER' 'GUARD'\n",
      " 'HAN' 'HAN and LUKE' 'HAN/PILOT' 'HEAD CONTROLLER' 'HOBBIE' 'HUMAN'\n",
      " 'IMPERIAL OFFICER' 'IMPERIAL SOLDIER' 'INTERCOM VOICE' 'JABBA' 'JANSON'\n",
      " 'JERJERROD' 'LANDO' 'LEIA' 'LIEUTENANT' 'LUKE' 'LURE' 'MAN'\n",
      " 'MASSASSI INTERCOM VOICE' 'MEDICAL DROID' 'MON MOTHMA' 'MOTTI'\n",
      " 'NAVIGATOR' 'NEEDA' 'NINEDENINE' 'OFFICER' 'OFFICER CASS' 'OOLA'\n",
      " 'OPERATOR' 'OWEN' 'OZZEL' 'PIETT' 'PILOT' 'PILOT #2' 'PILOT VOICE'\n",
      " 'PILOTS' 'PORKINS' 'REBEL CAPTAIN' 'REBEL FIGHTER' 'REBEL OFFICER'\n",
      " 'REBEL PILOT' 'RED ELEVEN' 'RED LEADER' 'RED NINE' 'RED SEVEN' 'RED TEN'\n",
      " 'RED THREE' 'RED TWO' 'RIEEKAN' 'SCOUT' 'SCOUT #1' 'SCOUT #2' 'SCOUT #l'\n",
      " 'SECOND COMMANDER' 'SECOND CONTROLLER' 'SECOND OFFICER' 'SECOND THREEPIO'\n",
      " 'SECOND TROOPER' 'SENIOR CONTROLLER' 'SHUTTLE CAPTAIN' 'STORMTROOPER'\n",
      " 'STRANGE VOICE' 'TAGGE' 'TARKIN' 'TECHNICIAN' 'THREEPIO'\n",
      " 'TRACKING OFFICER' 'TRENCH OFFICER' 'TROOPER' 'TROOPER VOICE' 'VADER'\n",
      " 'VEERS' 'VOICE' 'VOICE OVER DEATH STAR INTERCOM' 'WALKER PILOT #1'\n",
      " 'WEDGE' 'WILLARD' 'WINGMAN' 'WOMAN' 'WOMAN CONTROLLER' 'Y-WING PILOT'\n",
      " 'YODA' 'ZEV']\n"
     ]
    }
   ],
   "source": [
    "# all characters\n",
    "all_characters_arr = np.unique(np.append(np.append(characters_ep4, characters_ep5), characters_ep6))\n",
    "\n",
    "print_header(\"All characters\")\n",
    "print_arr(all_characters_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "-- All recurring characters (that are in every episode) --\n",
      "----------------------------------------------------------\n",
      "size = 8\n",
      "['THREEPIO' 'LUKE' 'VADER' 'LEIA' 'BEN' 'HAN' 'OFFICER' 'WEDGE']\n"
     ]
    }
   ],
   "source": [
    "# characters in every episode\n",
    "all_trainable_characters_arr = np.array([])\n",
    "\n",
    "for character in characters_ep4:\n",
    "    if character in characters_ep5 and character in characters_ep6:\n",
    "        all_trainable_characters_arr = np.append(all_trainable_characters_arr, character)\n",
    "\n",
    "print_header(\"All recurring characters (that are in every episode)\")\n",
    "print_arr(all_trainable_characters_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "-- Recurring characters (sorted by most dialogue) --\n",
      "----------------------------------------------------\n",
      "size = 8\n",
      "LUKE        494\n",
      "HAN         459\n",
      "THREEPIO    301\n",
      "LEIA        227\n",
      "VADER       140\n",
      "BEN         115\n",
      "WEDGE        33\n",
      "OFFICER      13\n",
      "Name: character, dtype: int64\n",
      "\n",
      "----------------------------------------------------\n",
      "-- Trainable characters (sorted by most dialogue) --\n",
      "----------------------------------------------------\n",
      "size = 6\n",
      "LUKE        494\n",
      "HAN         459\n",
      "THREEPIO    301\n",
      "LEIA        227\n",
      "VADER       140\n",
      "BEN         115\n",
      "Name: character, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check how many times a recurring character says something\n",
    "trainable_character_counts = (\n",
    "    df_ep4.character.value_counts()[all_trainable_characters_arr]\n",
    "    + df_ep5.character.value_counts()[all_trainable_characters_arr]\n",
    "    + df_ep6.character.value_counts()[all_trainable_characters_arr])\n",
    "\n",
    "trainable_character_counts = trainable_character_counts.sort_values(ascending=False)\n",
    "\n",
    "print_header(\"Recurring characters (sorted by most dialogue)\")\n",
    "print_arr(trainable_character_counts)\n",
    "\n",
    "# narrow down trainable characters\n",
    "trainable_character_counts = trainable_character_counts[:6]\n",
    "\n",
    "print(\"\")\n",
    "print_header(\"Trainable characters (sorted by most dialogue)\")\n",
    "print_arr(trainable_character_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "! CHARACTER TO TRAIN = HAN !\n",
      "\n",
      "-----------------\n",
      "-- HAN's lines --\n",
      "-----------------\n",
      "Episode IV:\n",
      "size = 153\n",
      "     character                                           dialogue\n",
      "342        HAN  Han Solo.  I'm captain of the Millennium Falco...\n",
      "344        HAN  Fast ship?  You've never heard of the Millenni...\n",
      "346        HAN  It's the ship that made the Kessel run in less...\n",
      "347        HAN  I've outrun Imperial starships, not the local ...\n",
      "349        HAN           What is it?  Some kind of local trouble?\n",
      "...        ...                                                ...\n",
      "995        HAN                             You're all clear, kid.\n",
      "996        HAN             Now let's blow this thing and go home!\n",
      "999        HAN       Great shot, kid.  That was one in a million.\n",
      "1002       HAN                                         Hey!  Hey!\n",
      "1004       HAN  Well, I wasn't gonna let you get all the credi...\n",
      "\n",
      "[153 rows x 2 columns]\n",
      "Episode V:\n",
      "size = 182\n",
      "    character                                           dialogue\n",
      "2         HAN                    Loud and clear, kid. What's up?\n",
      "4         HAN  There isn't enough life on this ice cube to fi...\n",
      "8         HAN                                            Chewie!\n",
      "9         HAN  All right, don't lose your temper. I'll come r...\n",
      "11        HAN  No sign of life out there, General. The sensor...\n",
      "..        ...                                                ...\n",
      "718       HAN                           What's going on...buddy?\n",
      "723       HAN          Stop, Chewie, stop! Do you hear me? Stop!\n",
      "725       HAN           Chewie! Chewie, this won't help me. Hey!\n",
      "726       HAN  Save your strength. There'll be another time. ...\n",
      "728       HAN                                            I know.\n",
      "\n",
      "[182 rows x 2 columns]\n",
      "Episode VI:\n",
      "size = 124\n",
      "    character                                           dialogue\n",
      "77        HAN                                       I can't see.\n",
      "79        HAN                                        Where am I?\n",
      "81        HAN                                       Who are you?\n",
      "83        HAN                                              Leia!\n",
      "85        HAN                    What's that? I know that laugh.\n",
      "..        ...                                                ...\n",
      "612       HAN                                         Stay back.\n",
      "613       HAN  Chewie!  Get down here!  She's wounded!  No, w...\n",
      "630       HAN                           Throw me another charge.\n",
      "639       HAN                                        Move! Move!\n",
      "673       HAN                                           Lando...\n",
      "\n",
      "[124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# choose which character the chatbot will train to mimic\n",
    "CHARACTER_NAME = 'HAN' # CHARACTER_NAME = character to train\n",
    "\n",
    "print(\"\")\n",
    "print(\"! \" + \"CHARACTER TO TRAIN = \" + CHARACTER_NAME + \" !\")\n",
    "print(\"\")\n",
    "\n",
    "print_header(CHARACTER_NAME + \"'s lines\")\n",
    "print(\"Episode IV:\")\n",
    "print_arr(df_ep4.loc[df_ep4.character == CHARACTER_NAME])\n",
    "print(\"Episode V:\")\n",
    "print_arr(df_ep5.loc[df_ep5.character == CHARACTER_NAME])\n",
    "print(\"Episode VI:\")\n",
    "print_arr(df_ep6.loc[df_ep6.character == CHARACTER_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Prepare data #\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_dataframe(df):\n",
    "    context_arr = []\n",
    "    n = 7 # context window size\n",
    "    \n",
    "    # loop over all CHARACTER_NAME's lines and add the previous dialogue as context\n",
    "    for i in df.loc[df.character == CHARACTER_NAME].index:\n",
    "      if i < n:\n",
    "        continue\n",
    "      row = []\n",
    "      prev = i - 1 - n # -1 => 6 context + 1 response\n",
    "      for j in range(i, prev, -1):\n",
    "        row.append(df.dialogue[j])\n",
    "      context_arr.append(row)\n",
    "    \n",
    "    columns = ['response', 'context'] \n",
    "    columns = columns + ['context/' + str(i) for i in range(n - 1)]\n",
    "    \n",
    "    return pd.DataFrame.from_records(context_arr, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 8)\n",
      "(180, 8)\n",
      "(124, 8)\n",
      "expected total length:  457\n",
      "final shape:  (457, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Han Solo.  I'm captain of the Millennium Falco...</td>\n",
       "      <td>I don't like the look of this.</td>\n",
       "      <td>This is Chewbacca.  He's first-mate on a ship ...</td>\n",
       "      <td>No blasters!  No blaster!</td>\n",
       "      <td>This little one isn't worth the effort.  Come ...</td>\n",
       "      <td>You'll be dead.</td>\n",
       "      <td>I'll be careful than.</td>\n",
       "      <td>Don't insult us.  You just watch yourself.  We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast ship?  You've never heard of the Millenni...</td>\n",
       "      <td>Yes, indeed.  If it's a fast ship.</td>\n",
       "      <td>Han Solo.  I'm captain of the Millennium Falco...</td>\n",
       "      <td>I don't like the look of this.</td>\n",
       "      <td>This is Chewbacca.  He's first-mate on a ship ...</td>\n",
       "      <td>No blasters!  No blaster!</td>\n",
       "      <td>This little one isn't worth the effort.  Come ...</td>\n",
       "      <td>You'll be dead.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Han Solo.  I'm captain of the Millennium Falco...   \n",
       "1  Fast ship?  You've never heard of the Millenni...   \n",
       "\n",
       "                              context  \\\n",
       "0      I don't like the look of this.   \n",
       "1  Yes, indeed.  If it's a fast ship.   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  This is Chewbacca.  He's first-mate on a ship ...   \n",
       "1  Han Solo.  I'm captain of the Millennium Falco...   \n",
       "\n",
       "                        context/1  \\\n",
       "0       No blasters!  No blaster!   \n",
       "1  I don't like the look of this.   \n",
       "\n",
       "                                           context/2  \\\n",
       "0  This little one isn't worth the effort.  Come ...   \n",
       "1  This is Chewbacca.  He's first-mate on a ship ...   \n",
       "\n",
       "                   context/3  \\\n",
       "0            You'll be dead.   \n",
       "1  No blasters!  No blaster!   \n",
       "\n",
       "                                           context/4  \\\n",
       "0                              I'll be careful than.   \n",
       "1  This little one isn't worth the effort.  Come ...   \n",
       "\n",
       "                                           context/5  \n",
       "0  Don't insult us.  You just watch yourself.  We...  \n",
       "1                                    You'll be dead.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "df_ep4_contextual = create_contextual_dataframe(df_ep4)\n",
    "df_ep5_contextual = create_contextual_dataframe(df_ep5)\n",
    "df_ep6_contextual = create_contextual_dataframe(df_ep6)\n",
    "\n",
    "print(df_ep4_contextual.shape)\n",
    "print(df_ep5_contextual.shape)\n",
    "print(df_ep6_contextual.shape)\n",
    "\n",
    "df = pd.concat([df_ep4_contextual, df_ep5_contextual, df_ep6_contextual])\n",
    "\n",
    "print(\"expected total length: \", df_ep4_contextual.shape[0] + df_ep5_contextual.shape[0] + df_ep6_contextual.shape[0])\n",
    "print(\"final shape: \", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>No, I need you to talk to the Falcon, find out...</td>\n",
       "      <td>Sir, I'm almost afraid to ask, but...does that...</td>\n",
       "      <td>I'm going to shut down everything but the emer...</td>\n",
       "      <td>Yes, lord.</td>\n",
       "      <td>Asteroids do not concern me, Admiral. I want  ...</td>\n",
       "      <td>Our ships have sighted the Millennium Falcon, ...</td>\n",
       "      <td>Yes, Admiral?</td>\n",
       "      <td>If you're saying coming here was a bad idea, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why do you take this apart now? I'm trying to ...</td>\n",
       "      <td>Oh, switch off.</td>\n",
       "      <td>Don't try to blame me. I didn't ask you to tur...</td>\n",
       "      <td>I can arrange that. You could use a good kiss!</td>\n",
       "      <td>I'd just as soon kiss a Wookiee.</td>\n",
       "      <td>Am I? Then why are you following me? Afraid I ...</td>\n",
       "      <td>You're imagining things.</td>\n",
       "      <td>No! That's not it. Come on. Aahhh -- uh huh! C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             response  \\\n",
       "94  No, I need you to talk to the Falcon, find out...   \n",
       "18  Why do you take this apart now? I'm trying to ...   \n",
       "\n",
       "                                              context  \\\n",
       "94  Sir, I'm almost afraid to ask, but...does that...   \n",
       "18                                    Oh, switch off.   \n",
       "\n",
       "                                            context/0  \\\n",
       "94  I'm going to shut down everything but the emer...   \n",
       "18  Don't try to blame me. I didn't ask you to tur...   \n",
       "\n",
       "                                         context/1  \\\n",
       "94                                      Yes, lord.   \n",
       "18  I can arrange that. You could use a good kiss!   \n",
       "\n",
       "                                            context/2  \\\n",
       "94  Asteroids do not concern me, Admiral. I want  ...   \n",
       "18                   I'd just as soon kiss a Wookiee.   \n",
       "\n",
       "                                            context/3  \\\n",
       "94  Our ships have sighted the Millennium Falcon, ...   \n",
       "18  Am I? Then why are you following me? Afraid I ...   \n",
       "\n",
       "                   context/4  \\\n",
       "94             Yes, Admiral?   \n",
       "18  You're imagining things.   \n",
       "\n",
       "                                            context/5  \n",
       "94  If you're saying coming here was a bad idea, I...  \n",
       "18  No! That's not it. Come on. Aahhh -- uh huh! C...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size=0.1, random_state=0)\n",
    "trn_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Training #\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-HAN'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3 # recommended 1 to 2 epochs\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small (3 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 03:45:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "01/29/2023 03:45:38 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x000001D635BA3A30>\n",
      "01/29/2023 03:45:38 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 03:45:38 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "C:\\Users\\Warre\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "01/29/2023 03:45:38 - INFO - __main__ -   ***** Running training *****\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Num examples = 411\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Num Epochs = 3\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "01/29/2023 03:45:38 - INFO - __main__ -     Total optimization steps = 306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31fc9e53a294faca2c952390c4060b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2f4053a72f461798695e3f203088dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3cd2149a43436b9f153f95fadb017a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69454f0245a427db9a5c689b041a32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 05:37:09 - INFO - __main__ -    global_step = 306, average loss = 2.5049088577819028\n",
      "01/29/2023 05:37:09 - INFO - __main__ -   Saving model checkpoint to output-small\n",
      "01/29/2023 05:37:19 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n",
      "01/29/2023 05:37:20 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 05:37:20 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "01/29/2023 05:37:20 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/29/2023 05:37:20 - INFO - __main__ -     Num examples = 46\n",
      "01/29/2023 05:37:20 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d1d5237d1b420fb7d51617359446c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 05:37:56 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/29/2023 05:37:56 - INFO - __main__ -     perplexity = tensor(7.4134)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(7.4134)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small-v2 (2 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:14:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "C:\\Users\\Warre\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1248: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "01/29/2023 16:14:06 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000021F892A7280>\n",
      "01/29/2023 16:14:06 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 16:14:06 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "C:\\Users\\Warre\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "01/29/2023 16:14:06 - INFO - __main__ -   ***** Running training *****\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Num examples = 411\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Num Epochs = 2\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "01/29/2023 16:14:06 - INFO - __main__ -     Total optimization steps = 204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d482cea0ebfe4a73a745cbd0d545928e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4d93edc948497f898bc60b8fb4db38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0622d2aae3ec4afaa33da7513bf48d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:14:27 - INFO - __main__ -    global_step = 204, average loss = 2.7952919801076255\n",
      "01/29/2023 16:14:27 - INFO - __main__ -   Saving model checkpoint to output-small-v2\n",
      "01/29/2023 16:14:30 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small-v2']\n",
      "01/29/2023 16:14:31 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 16:14:31 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "01/29/2023 16:14:31 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/29/2023 16:14:31 - INFO - __main__ -     Num examples = 46\n",
      "01/29/2023 16:14:31 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19c45e2baea4305b7f383495cf3f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:14:31 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/29/2023 16:14:31 - INFO - __main__ -     perplexity = tensor(9.1171)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(9.1171)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small-v3 (1 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:18:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "01/29/2023 16:18:14 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000021F9A5529D0>\n",
      "01/29/2023 16:18:14 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 16:18:15 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "01/29/2023 16:18:15 - INFO - __main__ -   ***** Running training *****\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Num examples = 411\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Num Epochs = 1\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "01/29/2023 16:18:15 - INFO - __main__ -     Total optimization steps = 102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee70b2a4c20c4551bc9f9709764f7321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48f162928d945be90dc94b575b68801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:18:24 - INFO - __main__ -    global_step = 102, average loss = 3.2971490177453733\n",
      "01/29/2023 16:18:24 - INFO - __main__ -   Saving model checkpoint to output-small-v3\n",
      "01/29/2023 16:18:27 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small-v3']\n",
      "01/29/2023 16:18:28 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "01/29/2023 16:18:28 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "01/29/2023 16:18:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/29/2023 16:18:28 - INFO - __main__ -     Num examples = 46\n",
      "01/29/2023 16:18:28 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0089cc5c2864c68ae0358d7f9def49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2023 16:18:29 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/29/2023 16:18:29 - INFO - __main__ -     perplexity = tensor(12.7401)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(12.7401)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
